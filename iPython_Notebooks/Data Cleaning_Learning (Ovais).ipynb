{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Dependencies--#\n",
    "\n",
    "#-- Data Cleaning Libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#-- Data Visualization Libraries:\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns #--just in case we need it, probably won't\n",
    "\n",
    "\n",
    "#-- Web Scraping Libraries:\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Settings for accessing Website\n",
    "\n",
    "executable_path = {\"executable_path\": \"chromedriver.exe\"}\n",
    "\n",
    "browser = Browser ('chrome', **executable_path, headless=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Functions for Data Scraping and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-defining Variables\n",
    "\n",
    "#Empty DataFrame for NBA\n",
    "NBA_game_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "#Reference to names\n",
    "NBA_refer = \"basketball-reference\"\n",
    "\n",
    "\n",
    "#URL\n",
    "NBA_game_url = \"https://www.basketball-reference.com\"\n",
    "\n",
    "#Create Empty List for years we want to scrape for\n",
    "years = [\"1990\", \"1991\", \"1992\", \"1993\", \"1994\", \"1995\",\\\n",
    "         \"1996\", \"1997\", \"1998\", \"1999\", \"2000\",\"2001\", \\\n",
    "         \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\",\\ \n",
    "         \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \\\n",
    "         \"2014\", \"2015\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our function for scraping NBA Data---> ** need to come back to this and finish it off\n",
    "def scrape_nba_data(page_url):\n",
    "    #URL's for both NBA and NCAA\n",
    "\n",
    "    for url in web_urls:\n",
    "        for year in years[0:2]:\n",
    "        \n",
    "            #Condition to check if the Web page is for the NBA \n",
    "            if NBA_refer in url:\n",
    "                \n",
    "                #set the rest of the url\n",
    "                url = f\"https://www.basketball-reference.com/leagues/NBA_{year}_per_game.html\"\n",
    "                \n",
    "#                 #Visit the NBA Web page\n",
    "#                 browser.visit(url)\n",
    "                \n",
    "#                 #Retrieve the html for the web page\n",
    "#                 html = browser.html\n",
    "                \n",
    "#                 #Clean Data by using predefined function: clean_nba_data\n",
    "#                 clean_nba_data(html)\n",
    "            \n",
    "                #hoooldd on wait a second, let me put some sleep in it\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                #Set the rest of the url\n",
    "                url = f\"https://www.sports-reference.com/cbb/play-index/psl_finder.cgi?request=1&match=single&year_min={year}&class_is_fr=Y&class_is_so=Y&class_is_jr=Y&class_is_sr=Y&pos_is_g=Y&pos_is_gf=Y&pos_is_f=Y&pos_is_fg=Y&pos_is_fc=Y&pos_is_c=Y&pos_is_cf=Y&games_type=A&c1stat=pf&c1comp=gt&order_by=year_id&order_by_asc=Y\"\n",
    "                \n",
    "                #visit the NCAA Web page\n",
    "                browser.visit(url)\n",
    "                \n",
    "                #retrive the html for the web page\n",
    "                html = browser.html\n",
    "                \n",
    "                #Use pandas to read html\n",
    "                year_html = pd.read_html(html, header = 0)\n",
    "                \n",
    "                print(year_html)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for cleaning and merging NBA data\n",
    "\n",
    "def clean_nba_data (page_html):\n",
    "    #Use pandas to read html\n",
    "    year_html = pd.read_html(page_html, header = 0)\n",
    "\n",
    "    #Get the second Table with all of the Data\n",
    "    year_html = year_html[1]\n",
    "\n",
    "    #Convert into DataFrame\n",
    "    year_df = pd.DataFrame(year_html)\n",
    "\n",
    "    #Delete Column of Ranking: \"Rk\"\n",
    "    year_df = year_df.rename(columns=({\"Rk\" : \"Year\"}))\n",
    "\n",
    "    #Apply the year to the Year column for each row\n",
    "    year_df[\"Year\"] = year_df[\"Year\"].apply(lambda x: year)\n",
    "                \n",
    "    #Calculate PER\n",
    "\n",
    "    print(year_df.head())\n",
    "                \n",
    "    #Reference global variable: NBA_game_df\n",
    "    global NBA_game_df\n",
    "\n",
    "    #Append to main DataFrame: NBA_game_df\n",
    "    if NBA_game_df.empty: \n",
    "        NBA_game_df = year_df\n",
    "    else: \n",
    "        NBA_game_df.append(year_df, ignore_index = True)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NCAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predefining some variables\n",
    "NCAA_df = pd.DataFrame()\n",
    "NCAA_refer = \"sports-reference\"\n",
    "NCAA_url = \"https://www.sports-reference.com/cbb/play-index/psl_finder.cgi?request=1&match=single&year_min=1993&year_max=2016&class_is_fr=Y&class_is_so=Y&class_is_jr=Y&class_is_sr=Y&pos_is_g=Y&pos_is_gf=Y&pos_is_f=Y&pos_is_fg=Y&pos_is_fc=Y&pos_is_c=Y&pos_is_cf=Y&games_type=A&c1stat=fga&c1comp=gt&order_by=year_id&order_by_asc=Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for scraping NCAA data\n",
    "def scrape_ncaa_data(page_url):\n",
    "    global NCAA_df\n",
    "    url = page_url\n",
    "    browser.visit(url)\n",
    "    click_count = 0\n",
    "    \n",
    "    while NCAA_refer in url:\n",
    "        try:\n",
    "            if click_count == 0:\n",
    "\n",
    "                #Get html of web page\n",
    "                html = browser.html\n",
    "\n",
    "                #Read html using pandas\n",
    "                read_html = pd.read_html(html)\n",
    "\n",
    "                #Create DF and append to ncaa_df\n",
    "                page_df = pd.DataFrame(read_html[1])\n",
    "                NCAA_df = page_df\n",
    "\n",
    "                #Add to click counter\n",
    "                click_count += 1\n",
    "                \n",
    "                #Sleep\n",
    "                time.sleep(1)\n",
    "                \n",
    "                #click to go to next page\n",
    "                next_page = browser.find_link_by_text(\"Next page\")\n",
    "                next_page.click()\n",
    "                \n",
    "\n",
    "\n",
    "            else:\n",
    "                #Get html of web page\n",
    "                html = browser.html\n",
    "\n",
    "                #Read html using pandas\n",
    "                read_html = pd.read_html(html)\n",
    "\n",
    "                #print(read_html[1])\n",
    "                page_df = pd.DataFrame(read_html[1])\n",
    "                NCAA_df = NCAA_df.append(page_df, ignore_index= True)\n",
    "\n",
    "                #Sleep\n",
    "                time.sleep(1)\n",
    "            \n",
    "                #click to go to next page\n",
    "                next_page = browser.find_link_by_text(\"Next page\")\n",
    "                next_page.click()\n",
    "        except AttributeError as error:\n",
    "            print(\"End of scrape\")\n",
    "            window.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for cleaning and merging NCAA data ** Will work on this later\n",
    "\n",
    "def clean_ncaa_data(ncaa_df):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping for Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NCAA\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of scrape\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'window' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mElementDoesNotExist\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mElementDoesNotExist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     45\u001b[0m                 u'no elements could be found with {0} \"{1}\"'.format(\n\u001b[1;32m---> 46\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                 )\n",
      "\u001b[1;31mElementDoesNotExist\u001b[0m: no elements could be found with link by text \"Next page\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'click'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-e18cf4229b23>\u001b[0m in \u001b[0;36mscrape_ncaa_data\u001b[1;34m(page_url)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mnext_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_link_by_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Next page\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                 \u001b[0mnext_page\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     82\u001b[0m                     u\"'{0}' object has no attribute '{1}'\".format(\n\u001b[1;32m---> 83\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ElementList' object has no attribute 'click'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-5a511fb52622>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscrape_ncaa_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNCAA_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-54-e18cf4229b23>\u001b[0m in \u001b[0;36mscrape_ncaa_data\u001b[1;34m(page_url)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"End of scrape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'window' is not defined"
     ]
    }
   ],
   "source": [
    "scrape_ncaa_data(NCAA_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test print\n",
    "NCAA_df\n",
    "NCAA_copy = NCAA_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Push Raw Data to CSV (So we don't have to scrape again)\n",
    "\n",
    "NCAA_copy.to_csv(\"NCAA_raw.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equation for calculating PER??????/\n",
    "\n",
    "PER_calc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NCAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
